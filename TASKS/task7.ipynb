{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM-based NMT  SAMPLES**"
      ],
      "metadata": {
        "id": "bgRMJ8deoM5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzspII--oE6N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "data = {\n",
        "    'english': ['hello', 'how are you', 'good morning', 'thank you'],\n",
        "    'hindi': ['नमस्ते', 'आप कैसे हैं', 'शुभ प्रभात', 'धन्यवाद']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "tokenizer_en = Tokenizer()\n",
        "tokenizer_hi = Tokenizer()\n",
        "\n",
        "tokenizer_en.fit_on_texts(df['english'])\n",
        "tokenizer_hi.fit_on_texts(df['hindi'])\n",
        "\n",
        "sequences_en = tokenizer_en.texts_to_sequences(df['english'])\n",
        "sequences_hi = tokenizer_hi.texts_to_sequences(df['hindi'])\n",
        "\n",
        "\n",
        "max_length_en = max(len(seq) for seq in sequences_en)\n",
        "max_length_hi = max(len(seq) for seq in sequences_hi)\n",
        "\n",
        "X = pad_sequences(sequences_en, maxlen=max_length_en, padding='post')\n",
        "y = pad_sequences(sequences_hi, maxlen=max_length_hi, padding='post')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer_en.word_index)+1, output_dim=64, input_length=max_length_en))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tokenizer_hi.word_index)+1, activation='softmax')))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "y = np.expand_dims(y, -1)\n",
        "model.fit(X, y, epochs=100)\n",
        "\n",
        "\n",
        "model.save('lstm_nmt_model.h5')\n",
        "\n",
        "import pickle\n",
        "with open('tokenizer_en.pkl', 'wb') as file:\n",
        "    pickle.dump(tokenizer_en, file)\n",
        "\n",
        "with open('tokenizer_hi.pkl', 'wb') as file:\n",
        "    pickle.dump(tokenizer_hi, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtrlkVNdofBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}